---
## Directory where etcd data stored
etcd_data_dir: /var/lib/etcd

## Directory where the binaries will be installed
bin_dir: /usr/local/bin

## The access_ip variable is used to define how other nodes should access
## the node.  This is used in flannel to allow other flannel nodes to see
## this node for example.  The access_ip is really useful AWS and Google
## environments where the nodes are accessed remotely by the "public" ip,
## but don't know about that address themselves.
# access_ip: 1.1.1.1


## External LB example config
## apiserver_loadbalancer_domain_name: "elb.some.domain"
# loadbalancer_apiserver:
#   address: 1.2.3.4
#   port: 1234

## Internal loadbalancers for apiservers
# loadbalancer_apiserver_localhost: true

## Local loadbalancer should use this port
## And must be set port 6443
nginx_kube_apiserver_port: 6443
## If nginx_kube_apiserver_healthcheck_port variable defined, enables proxy liveness check.
nginx_kube_apiserver_healthcheck_port: 8081

### OTHER OPTIONAL VARIABLES
## For some things, kubelet needs to load kernel modules.  For example, dynamic kernel services are needed
## for mounting persistent volumes into containers.  These may not be loaded by preinstall kubernetes
## processes.  For example, ceph and rbd backed volumes.  Set to true to allow kubelet to load kernel
## modules.
# kubelet_load_modules: false

## Upstream dns servers
# upstream_dns_servers:
#   - 8.8.8.8
#   - 8.8.4.4

## There are some changes specific to the cloud providers
## for instance we need to encapsulate packets with some network plugins
## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external'
## When openstack is used make sure to source in the openstack credentials
## like you would do when using openstack-client before starting the playbook.
## Note: The 'external' cloud provider is not supported.
## TODO(riverzhang): https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager
# cloud_provider:

## Set these proxy values in order to update package manager and docker daemon to use proxies
# http_proxy: ""
# https_proxy: ""

## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy
# no_proxy: ""

## Some problems may occur when downloading files over https proxy due to ansible bug
## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable
## SSL validation of get_url module. Note that kubespray will still be performing checksum validation.
# download_validate_certs: False

## If you need exclude all cluster nodes from proxy and other resources, add other resources here.
# additional_no_proxy: ""

## Certificate Management
## This setting determines whether certs are generated via scripts.
## Chose 'none' if you provide your own certificates.
## Option is  "script", "none"
## note: vault is removed
# cert_management: script

## Set to true to allow pre-checks to fail and continue deployment
# ignore_assert_errors: false

## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable.
# kube_read_only_port: 10255

## Set true to download and cache container
# download_container: true

## Deploy container engine
# Set false if you want to deploy container engine manually.
# deploy_container_engine: true

## Set Pypi repo and cert accordingly
# pyrepo_index: https://pypi.example.com/simple
# pyrepo_cert: /etc/ssl/certs/ca-certificates.crt

## Override specific ansible role default variables
#  #Hash is default value
## Kubernetes Master Node
## roles->kubernetes->master->defaults->main.yml
# Limits for kube components
kube_controller_memory_limit: 512M #512M
kube_controller_cpu_limit: 250m #250m
kube_controller_memory_requests: 100M #100M
kube_controller_cpu_requests: 100m #100m
kube_controller_node_monitor_grace_period: 40s #40s
kube_controller_node_monitor_period: 5s #5s
kube_controller_pod_eviction_timeout: 5m0s #5m0s
kube_controller_terminated_pod_gc_threshold: 12500 #12500
kube_scheduler_memory_limit: 512M #512M
kube_scheduler_cpu_limit: 250m #250m
kube_scheduler_memory_requests: 300M #170M
kube_scheduler_cpu_requests: 80m #80m
kube_apiserver_memory_limit: 2000M #2000M
kube_apiserver_cpu_limit: 800m #800m
kube_apiserver_memory_requests: 256M #256M
kube_apiserver_cpu_requests: 100m #100m
kube_apiserver_request_timeout: "1m0s" #"1m0s"


## Kubernetes Worker Node
## roles->kubernetes->node->defaults->main.yml
# Reserve this space for kube resources
kube_memory_reserved: 512M #256M
kube_cpu_reserved: 500m #100m
# Reservation for master hosts
kube_master_memory_reserved: 800M #512M
kube_master_cpu_reserved: 400m #200m

kubelet_status_update_frequency: 10s #10s

# Requests for nginx load balancer app
nginx_memory_requests: 32M #32M
nginx_cpu_requests: 25m #25m


## Kubernetes Apps
## roles->kubernetes-apps->ansible->defaults->main.yml
# CoreDNS
# Limits for coredns
dns_memory_limit: 170Mi #170Mi
dns_cpu_requests: 100m #100m
dns_memory_requests: 70Mi #70Mi
dns_min_replicas: 3 #2
dns_nodes_per_replica: 16 #16
dns_cores_per_replica: 256 #256

# nodelocaldns
nodelocaldns_cpu_requests: 100m #100m
nodelocaldns_memory_limit: 170Mi #170Mi
nodelocaldnsdns_memory_requests: 70Mi #70Mi

# Netchecker
deploy_netchecker: false #false
netchecker_port: 31081 #31081
agent_report_interval: 15 #15
netcheck_namespace: default #default

# Limits for netchecker apps
netchecker_agent_cpu_limit: 30m #30m
netchecker_agent_memory_limit: 100M #100M
netchecker_agent_cpu_requests: 15m #15m
netchecker_agent_memory_requests: 64M #64M
netchecker_server_cpu_limit: 100m #100m
netchecker_server_memory_limit: 256M #256M
netchecker_server_cpu_requests: 50m #50m
netchecker_server_memory_requests: 64M #64M

# SecurityContext when PodSecurityPolicy is enabled
netchecker_agent_user: 1000 #1000
netchecker_server_user: 1000 #1000
netchecker_agent_group: 1000 #1000
netchecker_server_group: 1000 #1000

# Dashboard
dashboard_enabled: true #true
dashboard_replicas: 1 #1

# Limits for dashboard
dashboard_cpu_limit: 100m #100m
dashboard_memory_limit: 256M #256M
dashboard_cpu_requests: 50m #50m
dashboard_memory_requests: 64M #64M


## Calico
## roles->network_plugin->calico->defaults->main.yml
# Limits for apps
calico_node_memory_limit: 500M #500M
calico_node_cpu_limit: 300m #300m
calico_node_memory_requests: 64M #64M
calico_node_cpu_requests: 150m #150m
calicoctl_memory_limit: 170M #170M
calicoctl_cpu_limit: 100m #100m
calicoctl_memory_requests: 32M #32M
calicoctl_cpu_requests: 250m #250m


## Flannel
## roles->network_plugin->flannel->defaults->main.yml
# Limits for apps
flannel_memory_limit: 500M #500M
flannel_cpu_limit: 300m #300m
flannel_memory_requests: 64M #64M
flannel_cpu_requests: 150m #150m
